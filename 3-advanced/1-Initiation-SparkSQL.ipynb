{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Falonne KPAMEGAN\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0e533da5815559c0bf6121cbae94175",
     "grade": false,
     "grade_id": "cell-7fa9ef9f0e824f8d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SparkSQL\n",
    "\n",
    "In this notebook, we introduce SparkSQL, Spark's interface for working with structured data. From Spark 2.0 and forward, this is the preferred way of implementing Spark code, as it contains all of the latest optimisations.\n",
    "\n",
    "PySpark benefits a lot from SparkSQL, as there is performance parity between Scala, Java, Python and R interfaces for Spark which use the same optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d9e8482a065f020b11bedd0770e02b2",
     "grade": false,
     "grade_id": "cell-6e5d231a205820d5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "Before running Spark code, we need to start a SparkSession instance. The following block will be common to every notebook so you can run your code.\n",
    "\n",
    "While your SparkSession is running, you can hit `http://localhost:4040` or `http://host.docker.internal:4040` to get an overview of your Spark local cluster and all operations ongoing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
       "        'APPDATA': 'C:\\\\Users\\\\fkpamegan\\\\AppData\\\\Roaming',\n",
       "        'CAMLIBS': 'C:\\\\Program Files\\\\darktable\\\\lib\\\\libgphoto2\\\\2.5.30',\n",
       "        'CHOCOLATEYINSTALL': 'C:\\\\ProgramData\\\\chocolatey',\n",
       "        'CHROME_CRASHPAD_PIPE_NAME': '\\\\\\\\.\\\\pipe\\\\crashpad_1828_UYZHCXBXEQTMZXTD',\n",
       "        'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
       "        'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMPUTERNAME': 'UL200013001',\n",
       "        'COMSPEC': 'C:\\\\windows\\\\system32\\\\cmd.exe',\n",
       "        'CONDA_ALLOW_SOFTLINKS': 'false',\n",
       "        'CONDA_DEFAULT_ENV': 'pyspark-tutorial',\n",
       "        'CONDA_EXE': 'C:\\\\Anaconda3\\\\Scripts\\\\conda.exe',\n",
       "        'CONDA_PREFIX': 'C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial',\n",
       "        'CONDA_PROMPT_MODIFIER': '(pyspark-tutorial) ',\n",
       "        'CONDA_PYTHON_EXE': 'C:\\\\Anaconda3\\\\python.exe',\n",
       "        'CONDA_ROOT': 'C:\\\\Anaconda3',\n",
       "        'CONDA_SHLVL': '1',\n",
       "        'CURL_CA_BUNDLE': 'C:\\\\Program Files\\\\PostgreSQL\\\\14\\\\ssl\\\\certs\\\\ca-bundle.crt',\n",
       "        'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData',\n",
       "        'EFC_12316': '0',\n",
       "        'ELECTRON_RUN_AS_NODE': '1',\n",
       "        'FPS_BROWSER_APP_PROFILE_STRING': 'Internet Explorer',\n",
       "        'FPS_BROWSER_USER_PROFILE_STRING': 'Default',\n",
       "        'GDAL_DATA': 'C:\\\\Program Files\\\\PostgreSQL\\\\14\\\\gdal-data',\n",
       "        'HOMEDRIVE': 'C:',\n",
       "        'HOMEPATH': '\\\\Users\\\\fkpamegan',\n",
       "        'IOLIBS': 'C:\\\\Program Files\\\\darktable\\\\lib\\\\libgphoto2_port\\\\0.12.1',\n",
       "        'JAVA_HOME': 'C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial\\\\Library',\n",
       "        'JAVA_HOME_CONDA_BACKUP': 'C:\\\\Program Files\\\\OpenJDK\\\\jdk-22.0.1',\n",
       "        'JPY_INTERRUPT_EVENT': '2692',\n",
       "        'LOCALAPPDATA': 'C:\\\\Users\\\\fkpamegan\\\\AppData\\\\Local',\n",
       "        'LOGONSERVER': '\\\\\\\\DCS22-P1',\n",
       "        'NUMBER_OF_PROCESSORS': '12',\n",
       "        'ONEDRIVE': 'C:\\\\Users\\\\fkpamegan\\\\OneDrive',\n",
       "        'OPENCV_DIR': 'C:\\\\opencv\\\\build\\\\bin',\n",
       "        'ORIGINAL_XDG_CURRENT_DESKTOP': 'undefined',\n",
       "        'OS': 'Windows_NT',\n",
       "        'PATH': 'c:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial;C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial;C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial\\\\Library\\\\usr\\\\bin;C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial\\\\Library\\\\bin;C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial\\\\Scripts;C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial\\\\bin;C:\\\\Anaconda3\\\\condabin;C:\\\\Program Files (x86)\\\\VMware\\\\VMware Player\\\\bin;C:\\\\Python312\\\\Scripts;C:\\\\Python312;C:\\\\Program Files\\\\Eclipse Adoptium\\\\jdk-21.0.3.9-hotspot\\\\bin;C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\javapath;C:\\\\windows\\\\system32;C:\\\\windows;C:\\\\windows\\\\System32\\\\Wbem;C:\\\\windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\windows\\\\System32\\\\OpenSSH;C:\\\\ProgramData\\\\chocolatey\\\\bin;C:\\\\Program Files\\\\dotnet;C:\\\\Program Files\\\\PowerShell\\\\7;C:\\\\Program Files\\\\gs\\\\gs10.03.1\\\\bin;C:\\\\Program Files\\\\OpenJDK\\\\jdk-22.0.1\\\\bin;C:\\\\Program Files\\\\VSCodium\\\\bin;C:\\\\Program Files (x86)\\\\BaseX\\\\bin;C:\\\\Program Files\\\\CMake\\\\bin;C:\\\\Users\\\\Administrateur\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;.;C:\\\\Program Files\\\\MKVToolNix;C:\\\\opencv\\\\build\\\\bin;C:\\\\data-integration;C:\\\\Program Files\\\\Docker\\\\Docker\\\\resources\\\\bin;C:\\\\Users\\\\fkpamegan\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\fkpamegan\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin;C:\\\\Users\\\\fkpamegan\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin',\n",
       "        'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW',\n",
       "        'PGDATA': 'C:\\\\Program Files\\\\PostgreSQL\\\\14\\\\data',\n",
       "        'PGSYSCONFDIR': 'C:\\\\PostgresConf',\n",
       "        'POSTGIS_ENABLE_OUTDB_RASTERS': '1',\n",
       "        'POSTGIS_GDAL_ENABLED_DRIVERS': 'GTiff PNG JPEG GIF XYZ DTED USGSDEM AAIGrid',\n",
       "        'POWERSHELL_DISTRIBUTION_CHANNEL': 'MSI:Windows 10 Education',\n",
       "        'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
       "        'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 158 Stepping 10, GenuineIntel',\n",
       "        'PROCESSOR_LEVEL': '6',\n",
       "        'PROCESSOR_REVISION': '9e0a',\n",
       "        'PROGRAMDATA': 'C:\\\\ProgramData',\n",
       "        'PROGRAMFILES': 'C:\\\\Program Files',\n",
       "        'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
       "        'PROGRAMW6432': 'C:\\\\Program Files',\n",
       "        'PROJ_LIB': 'C:\\\\Program Files\\\\PostgreSQL\\\\14\\\\share\\\\contrib\\\\postgis-3.4\\\\proj',\n",
       "        'PROMPT': '(pyspark-tutorial) $P$G',\n",
       "        'PSMODULEPATH': 'C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\windows\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules',\n",
       "        'PT8HOME': 'C:\\\\Program Files\\\\Cisco Packet Tracer 8.2.1',\n",
       "        'PUBLIC': 'C:\\\\Users\\\\Public',\n",
       "        'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1',\n",
       "        'PYTHONIOENCODING': 'utf-8',\n",
       "        'PYTHONUNBUFFERED': '1',\n",
       "        'PYTHONUTF8': '1',\n",
       "        'PYTHON_FROZEN_MODULES': 'on',\n",
       "        'SESSIONNAME': 'Console',\n",
       "        'SSL_CERT_FILE': 'C:\\\\Anaconda3\\\\envs\\\\pyspark-tutorial\\\\Library\\\\ssl\\\\cacert.pem',\n",
       "        'SYSTEMDRIVE': 'C:',\n",
       "        'SYSTEMROOT': 'C:\\\\windows',\n",
       "        'TEMP': 'C:\\\\Users\\\\FKPAME~1\\\\AppData\\\\Local\\\\Temp',\n",
       "        'TMP': 'C:\\\\Users\\\\FKPAME~1\\\\AppData\\\\Local\\\\Temp',\n",
       "        'USERDNSDOMAIN': 'ADLYON2.UNIV-LYON2.FR',\n",
       "        'USERDOMAIN': 'ADLYON2',\n",
       "        'USERDOMAIN_ROAMINGPROFILE': 'ADLYON2',\n",
       "        'USERNAME': 'fkpamegan',\n",
       "        'USERPROFILE': 'C:\\\\Users\\\\fkpamegan',\n",
       "        'VBOX_MSI_INSTALL_PATH': 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\',\n",
       "        'VSCODE_CODE_CACHE_PATH': 'C:\\\\Users\\\\fkpamegan\\\\AppData\\\\Roaming\\\\Code\\\\CachedData\\\\f1a4fb101478ce6ec82fe9627c43efbf9e98c813',\n",
       "        'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost',\n",
       "        'VSCODE_CWD': 'C:\\\\Users\\\\fkpamegan\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code',\n",
       "        'VSCODE_ESM_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess',\n",
       "        'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true',\n",
       "        'VSCODE_IPC_HOOK': '\\\\\\\\.\\\\pipe\\\\0a80eedd-1.95.3-main-sock',\n",
       "        'VSCODE_L10N_BUNDLE_LOCATION': '',\n",
       "        'VSCODE_NLS_CONFIG': '{\"userLocale\":\"en-us\",\"osLocale\":\"fr-fr\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"C:\\\\\\\\Users\\\\\\\\fkpamegan\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Microsoft VS Code\\\\\\\\resources\\\\\\\\app\\\\\\\\out\\\\\\\\nls.messages.json\",\"locale\":\"en-us\",\"availableLanguages\":{}}',\n",
       "        'VSCODE_PID': '1828',\n",
       "        'WINDIR': 'C:\\\\windows',\n",
       "        'ZES_ENABLE_SYSMAN': '1',\n",
       "        '_CONDA_OLD_CHCP': '850',\n",
       "        '__CONDA_OPENSLL_CERT_FILE_SET': '\"1\"',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       "        'PYSPARK_PYTHON': 'python',\n",
       "        'SPARK_AUTH_SOCKET_TIMEOUT': '15',\n",
       "        'SPARK_BUFFER_SIZE': '65536'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbe887d38fff8342d47e41abcd97211a",
     "grade": false,
     "grade_id": "cell-5be3741301fa4113",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://UL200013001.ADLYON2.UNIV-LYON2.FR:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lecture-lyon2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2690c8ca510>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('lecture-lyon2').setMaster('local')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dc4cbc9925a07f92a9d0150ddac48c2",
     "grade": false,
     "grade_id": "cell-7d0185de085b6093",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a1c1cd3e-1c50-4ab1-829f-f6f13634a0de\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"a1c1cd3e-1c50-4ab1-829f-f6f13634a0de\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.5.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.5.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.5.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.5.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.5.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a1c1cd3e-1c50-4ab1-829f-f6f13634a0de\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import other important libraries\n",
    "\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10a41f7082f75a43843018815e6b7d25",
     "grade": false,
     "grade_id": "cell-a37b40688b026b9a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Part A - On to DataFrames / Datasets\n",
    "\n",
    "A `Dataset` is a distributed collection of data which provides the benefits of RDDs (strong typing, ability to use lambda functions) with the benefits of SparkSQL's optimized execution engine.\n",
    "\n",
    "A `DataFrame` is a `Dataset` organized into named columns. It is conceptually equivalent to a table in a relational database, or a data frame in Python/R. Conceptually, a `DataFrame` is a `Dataset` of `Row`s.\n",
    "\n",
    "As with RDDs, applications can create DataFrames from an existing RDD, a Hive table or from Spark data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa12bb7ce18076896fa86bfa294d7691",
     "grade": false,
     "grade_id": "cell-e5336c5bf4eae8ef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Question\n",
    "\n",
    "Recall from the previous assignment how we used two tables on students : one for students to grades, another one for students to gender. Let's create a function which takes a RDD of Row and a schema as arguments and generates the corresponding DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def mean_grade_per_gender(sc, genders, grades):\\n    joined_rdd = genders.join(grades)\\n    gender_grades = joined_rdd.map(lambda x: (x[1][0], x[1][1]))\\n    sum_by_gender = gender_grades.reduceByKey(lambda x, y: x + y)\\n\\n    count_by_gender = gender_grades.countByKey()\\n\\n    patate = sum_by_gender.map(lambda gender_to_sum: (gender_to_sum[0] , gender_to_sum[1] / count_by_gender[gender_to_sum[0]])).sortByKey(ascending=False)\\n\\n    print(patate.collect())\\n\\n    return patate\\n\\n    raise NotImplementedError()'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def mean_grade_per_gender(sc, genders, grades):\n",
    "    joined_rdd = genders.join(grades)\n",
    "    gender_grades = joined_rdd.map(lambda x: (x[1][0], x[1][1]))\n",
    "    sum_by_gender = gender_grades.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    count_by_gender = gender_grades.countByKey()\n",
    "\n",
    "    patate = sum_by_gender.map(lambda gender_to_sum: (gender_to_sum[0] , gender_to_sum[1] / count_by_gender[gender_to_sum[0]])).sortByKey(ascending=False)\n",
    "\n",
    "    print(patate.collect())\n",
    "\n",
    "    return patate\n",
    "\n",
    "    raise NotImplementedError()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"genders_rdd = spark.sparkContext.parallelize([('1', 'M'), ('2', 'M'), ('3', 'F'), ('4', 'F'), ('5', 'F'), ('6', 'M')])\\ngrades_rdd = spark.sparkContext.parallelize([('1', 5), ('2', 12), ('3', 7), ('4', 18), ('5', 9), ('6', 5)])\\n\\nresult_rdd = mean_grade_per_gender(spark.sparkContext, genders_rdd, grades_rdd)\\nresult_rdd\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"genders_rdd = spark.sparkContext.parallelize([('1', 'M'), ('2', 'M'), ('3', 'F'), ('4', 'F'), ('5', 'F'), ('6', 'M')])\n",
    "grades_rdd = spark.sparkContext.parallelize([('1', 5), ('2', 12), ('3', 7), ('4', 18), ('5', 9), ('6', 5)])\n",
    "\n",
    "result_rdd = mean_grade_per_gender(spark.sparkContext, genders_rdd, grades_rdd)\n",
    "result_rdd\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a60e45e3c213939dc899b77a569322ea",
     "grade": false,
     "grade_id": "cell-33e39dc1e3d93fe9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_dataframe(spark, rdd, schema):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame from a RDD of Rows and a schema.\n",
    "    We assume the RDD is correctly formatted, no need to check for anything.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    df = spark.createDataFrame(rdd, schema)\n",
    "    return df\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62e4144642c715703e8343086ad95362",
     "grade": true,
     "grade_id": "cell-e8fd68e91290bbdc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "2 points\n",
    "\"\"\"\n",
    "rdd = spark.sparkContext.parallelize([('1', 'a'), ('2', 'b'), ('3', 'c'), ('4', 'd'), ('5', 'e'), ('6', 'f')])\n",
    "schema = StructType([StructField('ID', StringType(), True), StructField('letter', StringType(), True)])\n",
    "\n",
    "result_df = create_dataframe(spark, rdd, schema)\n",
    "assert result_df.schema == schema\n",
    "assert result_df.rdd.collect() == rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffa463e480a3796dc6840dade77c29d2",
     "grade": false,
     "grade_id": "cell-90f8ab4ce25e6789",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's generate a Dataframe of the students tables for the incoming questions, using our newly created `create_dataframe` function. We also create temporary views for those DataFrames so we can interact with them in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "576a688e54e8d23077804e3088373859",
     "grade": false,
     "grade_id": "cell-585b7ef68a2def27",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "genders_rdd = spark.sparkContext.parallelize([('1', 'M'), ('2', 'M'), ('3', 'F'), ('4', 'F'), ('5', 'F'), ('6', 'M')])\n",
    "grades_rdd = spark.sparkContext.parallelize([('1', 5), ('2', 12), ('3', 7), ('4', 18), ('5', 9), ('6', 5)])\n",
    "\n",
    "genders_schema = StructType([StructField('ID', StringType(), True), StructField('gender', StringType(), True)])\n",
    "grades_schema = StructType([StructField('ID', StringType(), True), StructField('grade', StringType(), True)])\n",
    "\n",
    "genders_df = create_dataframe(spark, genders_rdd, genders_schema)\n",
    "grades_df = create_dataframe(spark, grades_rdd, grades_schema)\n",
    "\n",
    "genders_df.createOrReplaceTempView('genders')\n",
    "grades_df.createOrReplaceTempView('grades')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43a31e2b3f52f535ebb0dc33f5d4600f",
     "grade": false,
     "grade_id": "cell-ed61ff691a07f8cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You have two ways of interacting with a Dataframe :\n",
    "\n",
    "* DataFrames provide a domain-specific language for structured manipulation :\n",
    "\n",
    "```python\n",
    ">> genders_df.filter(genders_df['ID'] > 2)\n",
    "+---+------+\n",
    "| ID|gender|\n",
    "+---+------+\n",
    "|  3|     F|\n",
    "|  4|     F|\n",
    "|  5|     F|\n",
    "|  6|     M|\n",
    "+---+------+\n",
    "```\n",
    "\n",
    "In the more simple cases, you can interact with DataFrames with a syntax close to the Pandas syntax.\n",
    "\n",
    "```python\n",
    ">> genders_df[genders_df['ID'] > 2]\n",
    "+---+------+\n",
    "| ID|gender|\n",
    "+---+------+\n",
    "|  3|     F|\n",
    "|  4|     F|\n",
    "|  5|     F|\n",
    "|  6|     M|\n",
    "+---+------+\n",
    "```\n",
    "\n",
    "* The `sql` function of a SparkSession enables to run SQL queries directly on the frame and returns a DataFrame, on which you can continue your computations\n",
    "\n",
    "```python\n",
    "# Register the DataFrame as a SQL temporary view beforehand\n",
    ">> genders_df.createOrReplaceTempView('genders')\n",
    ">> spark.sql('SELECT * FROM genders WHERE ID > 2').show()\n",
    "+---+------+\n",
    "| ID|gender|\n",
    "+---+------+\n",
    "|  3|     F|\n",
    "|  4|     F|\n",
    "|  5|     F|\n",
    "|  6|     M|\n",
    "+---+------+\n",
    "```\n",
    "\n",
    "Don't hesitate to check the [DataFrame Function Reference](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) for all of the operators you can use on a DataFrame. Use the following cell to experiment :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| ID|gender|\n",
      "+---+------+\n",
      "|  3|     F|\n",
      "|  4|     F|\n",
      "|  5|     F|\n",
      "|  6|     M|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to practice your new SQL skills\n",
    "genders_df[genders_df['ID'] > 2].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84dda8fca16bc681158a766f5c04a039",
     "grade": false,
     "grade_id": "cell-394f3276e415e799",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Question\n",
    "\n",
    "Remember the mean grade per gender question from last assignment ? Remember how unpleasant it was ? Let's do that directly in SparkSQL ! You can do it with whatever way pleases you between programmatic SQL or SparkSQL DSL. \n",
    "\n",
    "PS : if you are using programmatic SQL interaction, you may want to define a temporary view of temporary variables. You may want to delete those views at the end of your function with `spark.catalog.dropTempView('your_view')`. Be careful if removing the view, DataFrame are also lazily computed so don't delete your view if you still have not computed and cached the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8eac7b815cee4266095f8e25ba25bba3",
     "grade": false,
     "grade_id": "cell-f3ba785bb228f244",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_grade_per_gender(spark, genders_df, grades_df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of studentID to grades and studentID to gender, compute mean grade for each gender.\n",
    "    Assume all studentIDs are present in both DataFrames, making inner join possible, no need to check that.\n",
    "    Schema of output DataFrame should be gender, mean.\n",
    "    \"\"\"\n",
    "    # Register DataFrames as temporary views\n",
    "    genders_df.createOrReplaceTempView(\"genders\")\n",
    "    grades_df.createOrReplaceTempView(\"grades\")\n",
    "    \n",
    "    # Perform SQL query to compute the mean grade per gender\n",
    "    query = \"\"\"\n",
    "        SELECT g.gender, AVG(CAST(gr.grade AS DOUBLE)) AS mean\n",
    "        FROM genders g\n",
    "        JOIN grades gr\n",
    "        ON g.ID = gr.ID\n",
    "        GROUP BY g.gender\n",
    "    \"\"\"\n",
    "    result_df = spark.sql(query)\n",
    "    \n",
    "    # Drop the temporary views to clean up\n",
    "    spark.catalog.dropTempView(\"genders\")\n",
    "    spark.catalog.dropTempView(\"grades\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d57dca4e3d70a2debb12ad977a4b9a98",
     "grade": true,
     "grade_id": "cell-a3fc7ed7c358193e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "3 points\n",
    "\"\"\"\n",
    "result_df = mean_grade_per_gender(spark, genders_df, grades_df).toPandas()\n",
    "result_df.columns == ['gender', 'grade']\n",
    "\n",
    "assert result_df[result_df['gender'] == 'F'].values[0][1] - 11.3 < 0.1\n",
    "assert result_df[result_df['gender'] == 'M'].values[0][1] - 7.3 < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3ef06f8572566429eeb2d72a1ab47a3",
     "grade": false,
     "grade_id": "cell-f7f70906407004f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Part B - Descriptive statistics in SparkSQL\n",
    "\n",
    "Let's reload the `FL_insurance_sample.csv` file from last assignment and freely interact with it.\n",
    "\n",
    "# Question\n",
    "\n",
    "Load the file by giving a path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a8aaf77691064a2f0709ea674097c9d",
     "grade": false,
     "grade_id": "cell-499cfd4c4ed40d81",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(spark, path):\n",
    "    \"\"\"\n",
    "    Create a DataFrame by loading an external csv file.\n",
    "    Assume the file has a header, uses \" as double quote and , as delimiter.\n",
    "    Infer its schema automatically.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use Spark's read API to load the CSV\n",
    "        df = spark.read.csv(\n",
    "            path,\n",
    "            header=True,           # The file has a header\n",
    "            inferSchema=True,      # Automatically infer the schema\n",
    "            quote='\"',             # Double quote for escaping\n",
    "            sep=\",\"                # Comma as the delimiter\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # In case the file does not exist or any other error, return None\n",
    "        print(f\"Error reading file at {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a1eb9c16951a70094e2eb14164e8391",
     "grade": true,
     "grade_id": "cell-00468b3f4490d43b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "2 points\n",
    "\"\"\"\n",
    "file_path = 'FL_insurance_sample.csv'\n",
    "result_df = read_csv(spark, file_path)\n",
    "\n",
    "assert result_df.schema == StructType([\n",
    "    StructField('policyID',IntegerType(),True),\n",
    "    StructField('statecode',StringType(),True),\n",
    "    StructField('county',StringType(),True),\n",
    "    StructField('eq_site_limit',DoubleType(),True),\n",
    "    StructField('hu_site_limit',DoubleType(),True),\n",
    "    StructField('fl_site_limit',DoubleType(),True),\n",
    "    StructField('fr_site_limit',DoubleType(),True),\n",
    "    StructField('tiv_2011',DoubleType(),True),\n",
    "    StructField('tiv_2012',DoubleType(),True),\n",
    "    StructField('eq_site_deductible',DoubleType(),True),\n",
    "    StructField('hu_site_deductible',DoubleType(),True),\n",
    "    StructField('fl_site_deductible',DoubleType(),True),\n",
    "    StructField('fr_site_deductible',IntegerType(),True),\n",
    "    StructField('point_latitude',DoubleType(),True),\n",
    "    StructField('point_longitude',DoubleType(),True),\n",
    "    StructField('line',StringType(),True),\n",
    "    StructField('construction',StringType(),True),\n",
    "    StructField('point_granularity',IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53dae314d93d79d671059f9407a2c514",
     "grade": false,
     "grade_id": "cell-268bf2c0c12204ae",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Question\n",
    "\n",
    "Let's plot the number of different counties in a histogram, like in the previous assignment. We have imported the `bokeh` module for interactive plotting. To do that, return a Pandas a dataframe which contains, for each county, the number of its occurences in the dataset.\n",
    "\n",
    "_Hint: a Spark Dataframe is distributed on a number of workers, so it cannot be plotted as is. You will need to collect the data you want to plot back in the driver. The `toPandas` is usable to retrieve a Pandas local Dataframe, be careful to only use it on small Dataframes !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0afd6a03f49cb00d9acc6369d8cadc3a",
     "grade": false,
     "grade_id": "cell-25e140d92c569470",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "insurance_df = read_csv(spark, 'FL_insurance_sample.csv')\n",
    "insurance_df.createOrReplaceTempView('insurance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ae528e9603872cb6514fb54fe5492e4",
     "grade": false,
     "grade_id": "cell-ef312f5810eb4820",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count_county(spark, insurance_df):\n",
    "    \"\"\"\n",
    "    Return a Pandas DataFrame containing, for each county, the number of its occurrences in the dataset.\n",
    "    Schema of the DataFrame should be ['county', 'count'].\n",
    "    \"\"\"\n",
    "    # Group by 'county' and count occurrences\n",
    "    county_counts = (\n",
    "        insurance_df.groupBy(\"county\")\n",
    "        .count()  # Count occurrences for each county\n",
    "        .withColumnRenamed(\"count\", \"count\")  \n",
    "    )\n",
    "    \n",
    "    pandas_df = county_counts.toPandas()\n",
    "    \n",
    "    return pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f830073770b813cebfeefedda91abb9f",
     "grade": true,
     "grade_id": "cell-763e162bf7d285f6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "3 points\n",
    "\"\"\"\n",
    "df = count_county(spark, insurance_df)\n",
    "result = df.set_index('county').to_dict()['count']\n",
    "\n",
    "assert result.get('CLAY COUNTY') == 346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CALHOUN COUNTY</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLAY COUNTY</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PUTNAM COUNTY</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OKALOOSA COUNTY</td>\n",
       "      <td>1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUMTER COUNTY</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SUWANNEE COUNTY</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GULF COUNTY</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MADISON COUNTY</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>COLUMBIA COUNTY</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ST  JOHNS COUNTY</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FRANKLIN COUNTY</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DUVAL COUNTY</td>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MARION COUNTY</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BAKER COUNTY</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WASHINGTON COUNTY</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HAMILTON COUNTY</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JACKSON COUNTY</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BAY COUNTY</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ESCAMBIA COUNTY</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LEON COUNTY</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JEFFERSON COUNTY</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>VOLUSIA COUNTY</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>WALTON COUNTY</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LAFAYETTE COUNTY</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>WAKULLA COUNTY</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TAYLOR COUNTY</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BRADFORD COUNTY</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>UNION COUNTY</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SANTA ROSA COUNTY</td>\n",
       "      <td>856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NASSAU COUNTY</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ALACHUA COUNTY</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LIBERTY COUNTY</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GADSDEN COUNTY</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>HOLMES COUNTY</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>FLAGLER COUNTY</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LAKE COUNTY</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               county  count\n",
       "0      CALHOUN COUNTY     68\n",
       "1         CLAY COUNTY    346\n",
       "2       PUTNAM COUNTY    245\n",
       "3     OKALOOSA COUNTY   1115\n",
       "4       SUMTER COUNTY      9\n",
       "5     SUWANNEE COUNTY    154\n",
       "6         GULF COUNTY     72\n",
       "7      MADISON COUNTY     81\n",
       "8     COLUMBIA COUNTY    124\n",
       "9    ST  JOHNS COUNTY    657\n",
       "10    FRANKLIN COUNTY     37\n",
       "11       DUVAL COUNTY   1894\n",
       "12      MARION COUNTY     50\n",
       "13       BAKER COUNTY     70\n",
       "14  WASHINGTON COUNTY    116\n",
       "15    HAMILTON COUNTY     35\n",
       "16     JACKSON COUNTY    208\n",
       "17         BAY COUNTY    403\n",
       "18    ESCAMBIA COUNTY    494\n",
       "19        LEON COUNTY    246\n",
       "20   JEFFERSON COUNTY     57\n",
       "21     VOLUSIA COUNTY    833\n",
       "22      WALTON COUNTY    288\n",
       "23   LAFAYETTE COUNTY     68\n",
       "24     WAKULLA COUNTY     85\n",
       "25      TAYLOR COUNTY    113\n",
       "26    BRADFORD COUNTY     29\n",
       "27       UNION COUNTY      8\n",
       "28  SANTA ROSA COUNTY    856\n",
       "29      NASSAU COUNTY    135\n",
       "30     ALACHUA COUNTY    614\n",
       "31     LIBERTY COUNTY     36\n",
       "32     GADSDEN COUNTY    196\n",
       "33      HOLMES COUNTY     40\n",
       "34     FLAGLER COUNTY    204\n",
       "35        LAKE COUNTY     14"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = count_county(spark, insurance_df)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a7a5ff2274c9522c799cf3f1698171e",
     "grade": false,
     "grade_id": "cell-1e506728a67966a5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"a956e8e3-bf2a-46ae-bfac-e45e74ce6889\" data-root-id=\"p1050\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"50cc2001-7d7d-4607-bbda-566c8c787aa2\":{\"version\":\"3.5.2\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1050\",\"attributes\":{\"height\":250,\"x_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1060\",\"attributes\":{\"factors\":[\"CALHOUN COUNTY\",\"CLAY COUNTY\",\"PUTNAM COUNTY\",\"OKALOOSA COUNTY\",\"SUMTER COUNTY\",\"SUWANNEE COUNTY\",\"GULF COUNTY\",\"MADISON COUNTY\",\"COLUMBIA COUNTY\",\"ST  JOHNS COUNTY\",\"FRANKLIN COUNTY\",\"DUVAL COUNTY\",\"MARION COUNTY\",\"BAKER COUNTY\",\"WASHINGTON COUNTY\",\"HAMILTON COUNTY\",\"JACKSON COUNTY\",\"BAY COUNTY\",\"ESCAMBIA COUNTY\",\"LEON COUNTY\",\"JEFFERSON COUNTY\",\"VOLUSIA COUNTY\",\"WALTON COUNTY\",\"LAFAYETTE COUNTY\",\"WAKULLA COUNTY\",\"TAYLOR COUNTY\",\"BRADFORD COUNTY\",\"UNION COUNTY\",\"SANTA ROSA COUNTY\",\"NASSAU COUNTY\",\"ALACHUA COUNTY\",\"LIBERTY COUNTY\",\"GADSDEN COUNTY\",\"HOLMES COUNTY\",\"FLAGLER COUNTY\",\"LAKE COUNTY\"]}},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1052\",\"attributes\":{\"start\":0}},\"x_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1061\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1062\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1053\",\"attributes\":{\"text\":\"County counts\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1079\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1046\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1047\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1048\"},\"data\":{\"type\":\"map\",\"entries\":[[\"index\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAEAAAACAAAAAwAAAAQAAAAFAAAABgAAAAcAAAAIAAAACQAAAAoAAAALAAAADAAAAA0AAAAOAAAADwAAABAAAAARAAAAEgAAABMAAAAUAAAAFQAAABYAAAAXAAAAGAAAABkAAAAaAAAAGwAAABwAAAAdAAAAHgAAAB8AAAAgAAAAIQAAACIAAAAjAAAA\"},\"shape\":[36],\"dtype\":\"int32\",\"order\":\"little\"}],[\"county\",{\"type\":\"ndarray\",\"array\":[\"CALHOUN COUNTY\",\"CLAY COUNTY\",\"PUTNAM COUNTY\",\"OKALOOSA COUNTY\",\"SUMTER COUNTY\",\"SUWANNEE COUNTY\",\"GULF COUNTY\",\"MADISON COUNTY\",\"COLUMBIA COUNTY\",\"ST  JOHNS COUNTY\",\"FRANKLIN COUNTY\",\"DUVAL COUNTY\",\"MARION COUNTY\",\"BAKER COUNTY\",\"WASHINGTON COUNTY\",\"HAMILTON COUNTY\",\"JACKSON COUNTY\",\"BAY COUNTY\",\"ESCAMBIA COUNTY\",\"LEON COUNTY\",\"JEFFERSON COUNTY\",\"VOLUSIA COUNTY\",\"WALTON COUNTY\",\"LAFAYETTE COUNTY\",\"WAKULLA COUNTY\",\"TAYLOR COUNTY\",\"BRADFORD COUNTY\",\"UNION COUNTY\",\"SANTA ROSA COUNTY\",\"NASSAU COUNTY\",\"ALACHUA COUNTY\",\"LIBERTY COUNTY\",\"GADSDEN COUNTY\",\"HOLMES COUNTY\",\"FLAGLER COUNTY\",\"LAKE COUNTY\"],\"shape\":[36],\"dtype\":\"object\",\"order\":\"little\"}],[\"count\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"RAAAAFoBAAD1AAAAWwQAAAkAAACaAAAASAAAAFEAAAB8AAAAkQIAACUAAABmBwAAMgAAAEYAAAB0AAAAIwAAANAAAACTAQAA7gEAAPYAAAA5AAAAQQMAACABAABEAAAAVQAAAHEAAAAdAAAACAAAAFgDAACHAAAAZgIAACQAAADEAAAAKAAAAMwAAAAOAAAA\"},\"shape\":[36],\"dtype\":\"int32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1080\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1081\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"VBar\",\"id\":\"p1076\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"county\"},\"width\":{\"type\":\"value\",\"value\":0.9},\"top\":{\"type\":\"field\",\"field\":\"count\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"VBar\",\"id\":\"p1077\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"county\"},\"width\":{\"type\":\"value\",\"value\":0.9},\"top\":{\"type\":\"field\",\"field\":\"count\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"VBar\",\"id\":\"p1078\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"county\"},\"width\":{\"type\":\"value\",\"value\":0.9},\"top\":{\"type\":\"field\",\"field\":\"count\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1059\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1049\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"type\",\"@county\"],[\"count\",\"@count\"]]}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1068\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1069\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1070\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1071\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1063\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1064\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1065\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1066\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1067\",\"attributes\":{\"axis\":{\"id\":\"p1063\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1072\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1068\"}}}]}}]}};\n  const render_items = [{\"docid\":\"50cc2001-7d7d-4607-bbda-566c8c787aa2\",\"roots\":{\"p1050\":\"a956e8e3-bf2a-46ae-bfac-e45e74ce6889\"},\"root_ids\":[\"p1050\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1050"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot it for fun with bokeh.\n",
    "data = count_county(spark, insurance_df)\n",
    "\n",
    "source = ColumnDataSource(data)\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"type\", \"@county\"),\n",
    "    (\"count\", \"@count\"),\n",
    "])\n",
    "\n",
    "p = figure(x_range=data['county'].values, height=250, title=\"County counts\", tools=[hover])\n",
    "\n",
    "p.vbar(x='county', top='count', width=0.9, source=source)\n",
    "\n",
    "p.xgrid.grid_line_color = None\n",
    "p.y_range.start = 0\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cad25978528231cd59c728826d7adc0",
     "grade": false,
     "grade_id": "cell-7bfa0111bdc89374",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Postrequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f7237812e59b81cf93e7371b836c222",
     "grade": false,
     "grade_id": "cell-d620c45785d3135d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
